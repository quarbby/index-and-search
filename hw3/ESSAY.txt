1. Phrasal Queries 

We can handle phrasal queries by storing positional indices along the docID and term frequency. When we merge the postings lists, we find if the position of terms in the document are within k-terms of each other, where k is the distance the two terms are apart in the query phrase. 

We can also store n-gram models, or phrases in the document. So the document's features are both terms and phrases. For example, common phrases like "I love you" can be stored as a phrase in the model. We also build a bi-gram index. Then we compute the phrase as such:

- Process the query as a phrase query. 
- If phrase returns fewer than N results, split the query into bi-grams.
- Process the query as bi-gram queries. 
- If bi-grams still very little results, process each term independently.
- Merge the posting lists, while checking of the positional indices can match the distances the two terms are apart in the query phrase.
- If all the above fails, return no results.

But this is very computationally intensive. 

2. Long documents and queries will definitely take longer time to process. Additionally, long documents will have higher score due to the way we normalize. The longer the document, the higher the term frequency can be as there are more chance for a term to be included in a document. The implemented normalization won't address the issue, since upon normalization by Euclidean length, we loose information regarding length of a document. ltc.lnc is still not sufficient, since we still loose information regarding length of document, hence the said problem remains unsolved.

3. Yes.
Some field data is better than none and can be useful in determining the relevance of the document to the query. As not all the documents have metadata, we shouldn't place too much emphasis on it. So maybe we can give metadata lesser weightage than the result retrieved from the documents itself. 

Sometimes also the user might want to search for things in the metadata, like title, date, field (i.e. oil, gas)